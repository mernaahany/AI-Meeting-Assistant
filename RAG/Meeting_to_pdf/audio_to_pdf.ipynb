{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08fe3c9f-4aa4-44a5-9415-bfc20c50f3c0",
   "metadata": {},
   "source": [
    "## `libraries, files and models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95909859-9302-46a7-a571-67e2d424874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1644528-3a72-4115-9932-fc5bac343e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "from scipy.signal import butter, lfilter\n",
    "from pyannote.audio import Pipeline\n",
    "import torchaudio\n",
    "import torch\n",
    "from speechbrain.inference import SpeakerRecognition\n",
    "from speechbrain.utils.fetching import LocalStrategy\n",
    "from transformers import pipeline as hf_pipeline\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "from huggingface_hub import InferenceClient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bc4cb5-59bc-4658-8076-faf8475fafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "DB_PATH = os.path.join(BASE_DIR, \"speaker_database.npy\")\n",
    "ENROLL_DIR = os.path.join(BASE_DIR, \"processed_audio_enrollment\")\n",
    "TEST_DIR   = os.path.join(BASE_DIR, \"processed_audio_test\")\n",
    "PDF_DIR = os.path.join(BASE_DIR, \"docs\")\n",
    "os.makedirs(PDF_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f841bd8-4900-4fc9-a87b-bb91a5b58def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hp\\\\Downloads\\\\RAG\\\\Meeting_to_pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959caf21-9013-4454-ad99-8609db8e8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\"\n",
    "\n",
    "DIARIZATION_MODEL = \"pyannote/speaker-diarization-3.1\"\n",
    "ASR_MODEL = \"openai/whisper-small\"\n",
    "SPEAKER_DB_PATH = DB_PATH\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TARGET_SR = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb33c5c2-fdd7-4113-b237-b2a81fe655f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    token= HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206710f0-e968-471d-afb7-084d842c515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading diarization pipeline...\n",
      "Loading ASR model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading speaker recognition model...\n",
      "\n",
      " ====Models are loaded successfully====\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOAD MODELS\n",
    "# =========================\n",
    "print(\"Loading diarization pipeline...\")\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "    DIARIZATION_MODEL,\n",
    "    token=HF_TOKEN,\n",
    "    revision=\"main\"\n",
    ").to(torch.device(DEVICE))\n",
    "\n",
    "print(\"Loading ASR model...\")\n",
    "asr_pipeline = hf_pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=ASR_MODEL,\n",
    "    ignore_warning=True,\n",
    "    device=0 if DEVICE == \"cuda\" else -1,\n",
    "    generate_kwargs={\n",
    "        \"task\": \"transcribe\",\n",
    "        \"language\": \"en\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Loading speaker recognition model...\")\n",
    "speaker_verification = SpeakerRecognition.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    savedir=\"pretrained_models/spkrec\",\n",
    "    local_strategy=LocalStrategy.COPY_SKIP_CACHE\n",
    ")\n",
    "\n",
    "print(\"\\n ====Models are loaded successfully====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a264363a-5a6e-41d2-b031-16c7612d09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Speaker Database\n",
    "\n",
    "if os.path.exists(DB_PATH):\n",
    "    speaker_db = np.load(DB_PATH, allow_pickle=True).item()\n",
    "else:\n",
    "    speaker_db = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51cfcbb-9b50-4cf2-9a52-993ac7ab6350",
   "metadata": {},
   "source": [
    "## `Preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3497c85b-20a4-4177-890b-072a5d8c5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(audio, sr, low=80, high=7500):\n",
    "    b, a = butter(4, [low/(sr/2), high/(sr/2)], btype=\"band\")\n",
    "    return lfilter(b, a, audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "004be705-4ca2-43f7-9e98-5d0fcad2ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Silence Parts\n",
    "\n",
    "# def apply_vad(audio, sr, pad_ms=150):\n",
    "#     model, utils = torch.hub.load(\n",
    "#         \"snakers4/silero-vad\",\n",
    "#         \"silero_vad\",\n",
    "#         force_reload=False\n",
    "#     )\n",
    "#     get_speech_timestamps = utils[0]\n",
    "\n",
    "#     speech_timestamps = get_speech_timestamps(audio, model, sampling_rate=sr)\n",
    "\n",
    "#     pad = int(sr * pad_ms / 1000)\n",
    "#     segments = []\n",
    "\n",
    "#     for seg in speech_timestamps:\n",
    "#         start = max(0, seg[\"start\"] - pad)\n",
    "#         end = min(len(audio), seg[\"end\"] + pad)\n",
    "#         segments.append(audio[start:end])\n",
    "\n",
    "#     return np.concatenate(segments) if segments else audio\n",
    "\n",
    "def apply_vad(audio, sr):\n",
    "    model, utils = torch.hub.load(\n",
    "        repo_or_dir=\"snakers4/silero-vad\",\n",
    "        model=\"silero_vad\",\n",
    "        force_reload=False\n",
    "    )\n",
    "    get_speech_timestamps = utils[0]\n",
    "\n",
    "    speech_timestamps = get_speech_timestamps(audio, model, sampling_rate=sr)\n",
    "\n",
    "    speech_audio = []\n",
    "    for seg in speech_timestamps:\n",
    "        speech_audio.append(audio[seg[\"start\"]:seg[\"end\"]])\n",
    "\n",
    "    return np.concatenate(speech_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41170991-0c40-40f1-9f5c-3b8715f212af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(\n",
    "    input_wav,\n",
    "    output_dir,\n",
    "    suffix=\"_preprocessed\",\n",
    "    target_sr=16000\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(input_wav))[0]\n",
    "    output_wav = os.path.join(\n",
    "        output_dir,\n",
    "        f\"{base_name}{suffix}.wav\"\n",
    "    )\n",
    "\n",
    "    audio, sr = librosa.load(input_wav, sr=target_sr, mono=True)\n",
    "\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    audio = nr.reduce_noise(y=audio, sr=sr, prop_decrease=0.8)\n",
    "    audio = bandpass_filter(audio, sr)\n",
    "    audio = apply_vad(audio, sr)\n",
    "    audio = librosa.util.normalize(audio)\n",
    "\n",
    "    sf.write(output_wav, audio, sr)\n",
    "    return output_wav\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac75157-0186-4c0e-9029-013d402ea9da",
   "metadata": {},
   "source": [
    "## `Upload files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "506fd170-2241-4598-b26d-5c3954fabb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload WAV Files\n",
    "\n",
    "def select_audio_file():\n",
    "    root = Tk()\n",
    "    root.update()\n",
    "\n",
    "    file_path = askopenfilename(\n",
    "        title=\"Select an audio file\",\n",
    "        filetypes=[(\"WAV files\", \"*.wav\")]\n",
    "    )\n",
    "\n",
    "    root.destroy()\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4560e-e3a5-4d3d-b918-5b6fab775f86",
   "metadata": {},
   "source": [
    "## `Adding Speakers to the DB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f44781b-78c9-402b-938f-64b5854992b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Embeddings\n",
    "\n",
    "def extract_embedding(wav_path):\n",
    "    waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    if sr != 16000:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = speaker_verification.encode_batch(waveform.to(DEVICE))\n",
    "\n",
    "    return emb.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bd21008-f2a8-43d1-8889-5d7adbaedc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_speaker(audio_path):\n",
    "    global speaker_db\n",
    "\n",
    "    speaker_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "\n",
    "    processed_path = preprocess_audio(\n",
    "        input_wav=audio_path,\n",
    "        output_dir=ENROLL_DIR,\n",
    "        suffix=\"_enroll\"\n",
    "    )\n",
    "\n",
    "    embedding = extract_embedding(processed_path)\n",
    "\n",
    "    speaker_db[speaker_name] = embedding\n",
    "    np.save(DB_PATH, speaker_db)\n",
    "\n",
    "    print(f\"[✓] Speaker '{speaker_name}' added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9878c0ff-dce4-4a46-96d6-83e9eea50cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_speaker_from_dialog():\n",
    "    audio_path = select_audio_file()\n",
    "\n",
    "    if not audio_path:\n",
    "        print(\"[!] No file selected\")\n",
    "        return\n",
    "\n",
    "    add_speaker(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969fbb5e-4035-465d-967e-2d7df9b3dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hp/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Speaker 'Zira' added\n"
     ]
    }
   ],
   "source": [
    "add_speaker_from_dialog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee30e9b-9ac2-4444-b684-8347c285c273",
   "metadata": {},
   "source": [
    "## `Handling the Audio of the Meeting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "936e8792-47bd-43d1-8e7a-b2df43e0c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_audio(audio_path):\n",
    "    processed_path = preprocess_audio(\n",
    "        input_wav=audio_path,\n",
    "        output_dir=TEST_DIR,\n",
    "        suffix=\"_query\"\n",
    "    )\n",
    "\n",
    "    print(\"[✓] Audio processed successfully\")\n",
    "    return processed_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d8fcb4b-5950-443e-8583-b08cccc48f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LOAD AUDIO\n",
    "# =========================\n",
    "def upload_process_audio():\n",
    "    audio_path = select_audio_file()\n",
    "\n",
    "    if not audio_path:\n",
    "        print(\"[!] No file selected\")\n",
    "        return None\n",
    "\n",
    "    return process_query_audio(audio_path)\n",
    "\n",
    "def generate_transcription(DEVICE, SPEAKER_DB_PATH, ASR_MODEL, DIARIZATION_MODEL, HF_TOKEN, AUDIO_PATH, TARGET_SR= 16000):\n",
    "    # =========================\n",
    "    # LOAD AUDIO\n",
    "    # =========================\n",
    "    waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "    \n",
    "    if sample_rate != TARGET_SR:\n",
    "        waveform = torchaudio.functional.resample(\n",
    "            waveform, sample_rate, TARGET_SR\n",
    "        )\n",
    "        sample_rate = TARGET_SR\n",
    "    \n",
    "    # =========================\n",
    "    # HELPER FUNCTIONS\n",
    "    # =========================\n",
    "    def extract_segment(waveform, start, end, sr):\n",
    "        start_sample = int(start * sr)\n",
    "        end_sample = int(end * sr)\n",
    "        return waveform[:, start_sample:end_sample]\n",
    "    \n",
    "    def identify_speaker(segment, sample_rate):\n",
    "        min_duration_sec = 0.5\n",
    "        if segment.shape[1] < int(min_duration_sec * sample_rate):\n",
    "            return \"Unknown\", 0.0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            emb = speaker_verification.encode_batch(segment.to(DEVICE))\n",
    "            emb = emb.squeeze()\n",
    "    \n",
    "        best_speaker = \"Unknown\"\n",
    "        best_score = 0.0\n",
    "    \n",
    "        for name, ref_emb in speaker_db.items():\n",
    "            score = torch.nn.functional.cosine_similarity(\n",
    "                emb, torch.tensor(ref_emb).to(emb.device), dim=0\n",
    "            )\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_speaker = name\n",
    "    \n",
    "        if best_score < 0.3:\n",
    "            best_speaker = \"Unknown\"\n",
    "    \n",
    "        return best_speaker, float(best_score)\n",
    "    \n",
    "    \n",
    "    # =========================\n",
    "    # RUN DIARIZATION\n",
    "    # =========================\n",
    "    print(\"\\nRunning diarization...\")\n",
    "    diarization = diarization_pipeline({\n",
    "        \"waveform\": waveform,\n",
    "        \"sample_rate\": sample_rate\n",
    "    })\n",
    "    \n",
    "    print(\"\\n--- FINAL SPEAKER-ATTRIBUTED TRANSCRIPT ---\\n\")\n",
    "\n",
    "    transcript_lines = []\n",
    "    \n",
    "    for turn, _, _ in diarization.speaker_diarization.itertracks(yield_label=True):\n",
    "        start, end = turn.start, turn.end\n",
    "\n",
    "        segment = extract_segment(waveform, start, end, sample_rate)\n",
    "        speaker, _ = identify_speaker(segment, sample_rate)\n",
    "\n",
    "        segment_np = segment[0].cpu().numpy()\n",
    "        text = asr_pipeline(segment_np, chunk_length_s=0)[\"text\"].strip()\n",
    "\n",
    "        transcript_lines.append(\n",
    "            f\"{speaker}:\\n{text}\\n\"\n",
    "        )\n",
    "\n",
    "    full_transcript = \"\\n\".join(transcript_lines)\n",
    "\n",
    "    return full_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c650e56-a113-465e-9603-89a0406b85ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hp/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Audio processed successfully\n",
      "\n",
      "Running diarization...\n"
     ]
    }
   ],
   "source": [
    "AUDIO_PATH = upload_process_audio()\n",
    "\n",
    "if AUDIO_PATH is None:\n",
    "    print(\"[!] No audio to process\")\n",
    "else:\n",
    "    meeting_example= generate_transcription(\n",
    "        DEVICE,\n",
    "        SPEAKER_DB_PATH,\n",
    "        ASR_MODEL,\n",
    "        DIARIZATION_MODEL,\n",
    "        HF_TOKEN,\n",
    "        AUDIO_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7d360f2-23c7-4b64-94d4-6938988d25c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David2:\n",
      "Thank you for calling Union Mobile. My name is Ray. How can I assist you today?\n",
      "\n",
      "Zira2:\n",
      "Hi, I'm having some issues with my phone service. I've been experiencing dropped calls, poor reception, and data connectivity problems.\n",
      "\n",
      "David2:\n",
      "Sorry to hear that, Alyssa. Can you please verify your identity for me? I'll do my best to help you resolve the issue.\n",
      "\n",
      "Zira2:\n",
      "Sure, my account pin is 1234.\n",
      "\n",
      "David2:\n",
      "Thank you, Alyssa. I'm unable to verify your identity with the PU provided. Can you please try again?\n",
      "\n",
      "Zira2:\n",
      "Okay, let me check again. My account pin is 5678.\n",
      "\n",
      "David2:\n",
      "I apologize, but that doesn't seem to be working either. Can you please try one more time for me?\n",
      "\n",
      "Zira2:\n",
      "Alright, my account pin is 9,012.\n",
      "\n",
      "David2:\n",
      "Thank you, Alyssa. I'm now able to verify your identity. I see that you've had some issues with your phone service. Can you tell me a little bit more about what's been happening?\n",
      "\n",
      "Zira2:\n",
      "Yeah, like I said, I've been experiencing dropped calls, poor reception, and data connectivity problems. It's been really frustrating.\n",
      "\n",
      "David2:\n",
      "I understand how frustrating that must be for you. I may see what I can do to help. Can you tell me a little bit more about your phone and the areas where you're experiencing these issues?\n",
      "\n",
      "Zira2:\n",
      "Sure, I have an iPhone 12 Pro and I live in downtown Los Angeles.\n",
      "\n",
      "David2:\n",
      "Thank you for providing that information, Alyssa. Based on what you've told me, it sounds like there might be some network issues in your area. Let them going to go ahead and escalate your issue to our engineering team. They'll work on resolving the problem as soon as possible. In the meantime, I recommend like to offer you a complimentary one month subscription to our premium data plan, which will give you an extra abide of data per month. Would you like to take advantage of this offer?\n",
      "\n",
      "Zira2:\n",
      "Yes, that sounds great. Thank you.\n",
      "\n",
      "David2:\n",
      "You're welcome, Alyssa. Is there anything else I can assist you with today? No.\n",
      "\n",
      "Unknown:\n",
      "Thank you.\n",
      "\n",
      "Zira2:\n",
      "That's all. Thanks you so much for your help, Ray.\n",
      "\n",
      "David2:\n",
      "You're welcome, Alyssa. Have a great day and thank you for choosing Union Mobile.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(meeting_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f0f14-8322-41c0-ae67-cdbdc8d11e10",
   "metadata": {},
   "source": [
    "## `Generate the Report and Save it in PDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d1f7300-47a2-4bc2-9b5d-4b019680bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(points):\n",
    "    prompt = f\"\"\"\n",
    "You are a meeting report generator AI.\n",
    "\n",
    "Using the following extracted points:\n",
    "\n",
    "{points}\n",
    "\n",
    "Create a professional structured meeting report using this format:\n",
    "\n",
    "Meeting Title:\n",
    "Date:\n",
    "Participants:\n",
    "\n",
    "Summary:\n",
    "\n",
    "Key Discussion Points:\n",
    "- …\n",
    "\n",
    "Decisions Made:\n",
    "- …\n",
    "\n",
    "Action Items:\n",
    "- Person:\n",
    "  Task:\n",
    "  Deadline:\n",
    "\n",
    "Risks & Concerns:\n",
    "- …\n",
    "\n",
    "Next Meeting:\n",
    "- Date:\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=500,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06eaef91-2031-4d7a-a3de-9a0e4edf0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_text = generate_report(meeting_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "016edafb-a6ad-4a13-8d6e-1ac661454c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Meeting Title: Union Mobile Customer Support Call with Zira2\\nDate: [Current Date]\\nParticipants: David2 (Union Mobile Representative), Zira2 (Union Mobile Customer)\\n\\nSummary:\\nDuring the call, Zira2 reported experiencing issues with her phone service, including dropped calls, poor reception, and data connectivity problems. David2 attempted to verify Zira2's identity using her account pin but was unable to do so. After successfully verifying her identity, David2 gathered more information about the issues she was experiencing and discovered potential network issues in Zira2's area. He escalated the issue to the engineering team and offered her a complimentary one-month subscription to Union Mobile's premium data plan as a gesture of goodwill.\\n\\nKey Discussion Points:\\n- Zira2 reported experiencing dropped calls, poor reception, and data connectivity problems with her Union Mobile service.\\n- David2 was unable to verify Zira2's identity using the provided account pin.\\n- After successful verification, David2 gathered more information about the issues and discovered potential network issues in Zira2's area.\\n- The issue was escalated to the engineering team, and Zira2 was offered a complimentary one-month subscription to Union Mobile's premium data plan.\\n\\nDecisions Made:\\n- The issue was escalated to the engineering team for resolution.\\n- Zira2 was offered a complimentary one-month subscription to Union Mobile's premium data plan.\\n\\nAction Items:\\n- Engineering team: Investigate and resolve the network issues in Zira2's area.\\n  Deadline: [Expected Resolution Date]\\n\\nRisks & Concerns:\\n- The network issues in Zira2's area could potentially affect other customers in the same area.\\n- The engineering team may not be able to resolve the issue within the expected timeframe, potentially leading to continued frustration for Zira2.\\n\\nNext Meeting:\\n- No scheduled meeting at this time. However, an update on the engineering team's progress will be provided in a follow-up communication with Zira2.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "858ac03a-7afd-4cb0-bd53-0beadfd2ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meeting_title(report_text, fallback=\"Meeting_Report\"):\n",
    "    \"\"\"\n",
    "    Extracts the Meeting Title from the generated report.\n",
    "    Returns a filename-safe version.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"Meeting Title:\\s*(.+)\", report_text)\n",
    "\n",
    "    if not match:\n",
    "        return fallback\n",
    "\n",
    "    title = match.group(1).strip()\n",
    "\n",
    "    # Sanitize for file system\n",
    "    title = re.sub(r'[\\\\/*?:\"<>|]', \"\", title)  # remove illegal chars\n",
    "    title = re.sub(r\"\\s+\", \"_\", title)          # spaces → underscores\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a1c9879-4e1d-4740-b167-dbe52d5fc5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] PDF saved to C:\\Users\\hp\\Desktop\\Meeting assistant\\docs\\2025-12-18_Union_Mobile_Customer_Support_Call_with_Zira2.pdf\n"
     ]
    }
   ],
   "source": [
    "save_report_as_pdf(report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3264cb6c-4403-4a77-8b6d-2f6097ea2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, ListFlowable, ListItem\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.enums import TA_LEFT\n",
    "from reportlab.lib.units import inch\n",
    "import re\n",
    "\n",
    "# Save the Output in PDF\n",
    "\n",
    "def save_report_as_pdf(report_text):\n",
    "    \n",
    "    pdf_title = extract_meeting_title(report_text)\n",
    "\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    output_path = os.path.join(PDF_DIR, f\"{date_str}_{pdf_title}.pdf\")\n",
    "    \n",
    "    doc = SimpleDocTemplate(\n",
    "        output_path,\n",
    "        pagesize=A4,\n",
    "        rightMargin=40,\n",
    "        leftMargin=40,\n",
    "        topMargin=40,\n",
    "        bottomMargin=40\n",
    "    )\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "\n",
    "    header_style = ParagraphStyle(\n",
    "        \"Header\",\n",
    "        parent=styles[\"Normal\"],\n",
    "        fontSize=12,\n",
    "        fontName=\"Helvetica-Bold\",\n",
    "        spaceBefore=16,\n",
    "        spaceAfter=8,\n",
    "        alignment=TA_LEFT\n",
    "    )\n",
    "\n",
    "    body_style = ParagraphStyle(\n",
    "        \"Body\",\n",
    "        parent=styles[\"Normal\"],\n",
    "        fontSize=10,\n",
    "        spaceAfter=10,\n",
    "        leading=14\n",
    "    )\n",
    "\n",
    "    story = []\n",
    "\n",
    "    lines = report_text.split(\"\\n\")\n",
    "\n",
    "    bullet_buffer = []\n",
    "\n",
    "    def flush_bullets():\n",
    "        nonlocal bullet_buffer\n",
    "        if bullet_buffer:\n",
    "            story.append(\n",
    "                ListFlowable(\n",
    "                    [\n",
    "                        ListItem(\n",
    "                            Paragraph(item, body_style),\n",
    "                            leftIndent=18\n",
    "                        )\n",
    "                        for item in bullet_buffer\n",
    "                    ],\n",
    "                    bulletType=\"bullet\"\n",
    "                )\n",
    "            )\n",
    "            bullet_buffer = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if not line:\n",
    "            flush_bullets()\n",
    "            story.append(Spacer(1, 0.15 * inch))\n",
    "            continue\n",
    "\n",
    "        # Section headers\n",
    "        if re.match(\n",
    "            r\"^(Meeting Title|Date|Participants|Summary|Key Discussion Points|Decisions Made|Action Items|Risks & Concerns|Next Meeting):\",\n",
    "            line\n",
    "        ):\n",
    "            flush_bullets()\n",
    "            header = line.replace(\":\", \"\")\n",
    "            story.append(Paragraph(header, header_style))\n",
    "            continue\n",
    "\n",
    "        # Bullet points\n",
    "        if line.startswith(\"-\"):\n",
    "            bullet_buffer.append(line[1:].strip())\n",
    "            continue\n",
    "\n",
    "        # Normal paragraph\n",
    "        flush_bullets()\n",
    "        story.append(Paragraph(line, body_style))\n",
    "\n",
    "    flush_bullets()\n",
    "    doc.build(story)\n",
    "\n",
    "    print(f\"[✓] Formatted PDF saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "279f91df-d8e7-4ae2-ac06-a100130ef282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Formatted PDF saved to C:\\Users\\hp\\Desktop\\Meeting assistant\\docs\\2025-12-18_Union_Mobile_Customer_Support_Call_with_Zira2.pdf\n"
     ]
    }
   ],
   "source": [
    "save_report_as_pdf(report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f46958-9693-4688-b609-434063f9a757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hp\\\\anaconda3\\\\envs\\\\audio_rec\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22055444",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchaudio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchaudio'"
     ]
    }
   ],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed0c8e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5717044",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchaudio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchaudio'"
     ]
    }
   ],
   "source": [
    "import torchaudio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
